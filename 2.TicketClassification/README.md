# Ticket Classification with NLP

An end-to-end NLP project for automatic ticket classification, progressing from
classical machine learning baselines to modern transformer-based models.

The project is designed with a **production-oriented structure**, including:

- clean data loading and preprocessing
- reproducible training via CLI
- unit tests for core components
- model evaluation and comparison
- a FastAPI inference service
- containerized deployment with Docker

## üìå Project Overview

Customer support teams receive large volumes of free-text tickets every day.
This project focuses on automatically classifying support tickets into predefined product categories based on the textual description provided by users.

The project was developed incrementally, starting with a strong classical NLP baseline and evolving toward modern transformer-based models.

Key aspects of the project include:

- Classical NLP pipelines using **TF-IDF + linear classifiers**
- Reproducible training via CLI and centralized configuration
- Hyperparameter tuning and proper evaluation practices
- Fine-tuned **transformer-based models** (DistilBERT, RoBERTa)
- A **FastAPI-based inference service** for real-time predictions
- **Dockerized deployment** with a self-contained model image

The result is an end-to-end NLP system that reflects how text classification models are built,
evaluated, and deployed in real-world production settings.

## üìä Dataset

**Source:** Financial complaint/ticket dataset

**Input:** Free-text description (complaint_what_happened)

**Target:** Product category (product)

**Preprocessing:**

- Remove missing text or labels

- Keep the top 5 most frequent classes

- Divide data into train/validation/test subsets

### Data Availability

The dataset is **not included in this repository**.

To run training locally:

1. Download the dataset from:  
   üëâ https://www.kaggle.com/code/abhishek14398/automatic-ticket-classification-case-study-nlp/input

2. Unzip and place it in:

   üëâ your project root > data> raw

> For example: data/raw/tickets.json

## üß± Project Structure

```text
2.TicketClassification/
‚îú‚îÄ‚îÄ src/
‚îÇ ‚îî‚îÄ‚îÄ ticket_classifier/
‚îÇ ‚îú‚îÄ‚îÄ data_loader.py # Dataset loading and splitting
‚îÇ ‚îú‚îÄ‚îÄ pipeline.py # TF-IDF + classical ML pipelines
‚îÇ ‚îú‚îÄ‚îÄ train.py # CLI training for classical models
‚îÇ ‚îú‚îÄ‚îÄ train_transformer.py # Transformer training and evaluation
‚îÇ ‚îú‚îÄ‚îÄ evaluate.py # Evaluation utilities
‚îÇ ‚îú‚îÄ‚îÄ config.py # Centralized configuration
‚îÇ ‚îî‚îÄ‚îÄ init.py
‚îÇ
‚îú‚îÄ‚îÄ api/
‚îÇ ‚îú‚îÄ‚îÄ app.py # FastAPI inference service
‚îÇ ‚îú‚îÄ‚îÄ schemas.py # Request/response schemas
‚îÇ ‚îî‚îÄ‚îÄ main.py # Local API entry point
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ ‚îî‚îÄ‚îÄ raw/ # Raw dataset files (not tracked)
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ ‚îî‚îÄ‚îÄ *.joblib # Saved classical ML models (optional, local)
‚îÇ
‚îú‚îÄ‚îÄ artifacts/
‚îÇ ‚îî‚îÄ‚îÄ transformer_runs/ # Transformer checkpoints (not tracked)
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ ‚îú‚îÄ‚îÄ conftest.py # Pytest shared fixtures
‚îÇ ‚îú‚îÄ‚îÄ test_data_loader.py
‚îÇ ‚îú‚îÄ‚îÄ test_pipeline.py
‚îÇ ‚îú‚îÄ‚îÄ test_evaluate.py
‚îÇ ‚îî‚îÄ‚îÄ test_transformer_prep.py # Transformer data prep & sanity checks
‚îÇ
‚îú‚îÄ‚îÄ Dockerfile # Containerized inference service
‚îú‚îÄ‚îÄ requirements.txt # Runtime dependencies
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

## üß† Modeling Approach

### 1Ô∏è‚É£ Classical Machine Learning Models

**üîπText Representation**

    TFIDF Vectorization:

        - Configurable via config.py

        - N-grams: (1, 3)

        - Max_features: 10000

        - Stop-word removal (english)

This representation provides a strong, efficient baseline for linear classifiers on high-dimensional text data.

**üîπ Classifiers**

- Logistic Regression
  - class_weight="balanced"

- Linear SVM (LinearSVC)
  - class_weight="balanced"

Both models are implemented through a single configurable pipeline, ensuring consistent preprocessing, training, and evaluation.

**üîß Training & Hyperparameter Tuning**

Training is done via a command-line interface (CLI).

1. Training without tuning:

   PYTHONPATH=src python -m ticket_classifier.train --classifier logreg

   PYTHONPATH=src python -m ticket_classifier.train --classifier svc

2. Training with GridSearchCV:

   PYTHONPATH=src python -m ticket_classifier.train --classifier logreg --tune

   PYTHONPATH=src python -m ticket_classifier.train --classifier svc --tune

**üìà Evaluation**

Evaluation includes:

- Accuracy

- Precision / Recall / F1-score (per class)

- Macro & weighted averages

**Example performance (Top 5 classes):**

Accuracy: ~78%

Weighted F1: ~0.78

> These results are typical for TF-IDF-based models on semantically overlapping ticket categories and provide a strong baseline.

**Testing**

The project includes a pytest-based test covering:

- Pipeline construction

- TF-IDF configuration consistency

- Fit / predict sanity checks

- Data loading validation

- Integration tests on real data

> For running the tests: PYTHONPATH=src pytest

**‚öôÔ∏è Configuration**

All reusable parameters are centralized in config.py, including:

- TF-IDF settings

- Random seed

- Dataset split sizes

- Paths

> This ensures consistency across training, testing, and experiments.

### 2Ô∏è‚É£ Transformer-based Models

**üîπ Model Architectures**

Transformer-based models are fine-tuned for sequence classification using **Hugging Face**.

- Models explored: DistilBERT, RoBERTa

- Input: Raw ticket text

- Output:
  - predicted_label: Product category (Top 5 classes)
  - confidence score

**üîπ Tokenization**

- Model-specific tokenizers (AutoTokenizer)

- Truncation applied to long texts

- Configurable maximum sequence length (e.g., 128 tokens)

**üîß Transformer Training (CLI)**
Transformer fine-tuning is implemented in train_transformer.py and is fully configurable via CLI.

**Key features:**

- Fine-tuning with Hugging Face Trainer

- Configurable batch size, learning rate, epochs, and model choice

- Micro-training mode for fast iteration

- Automatic best-model selection based on weighted F1

**Train RoBERTa (full train/val split):**

```text
ACCELERATE_USE_CPU=1 PYTHONPATH=src python -m ticket_classifier.train_transformer \
 --model-name roberta-base --max-len 128 \
--train-n 11711 --val-n 1674 --epochs 3 --train-batch 4
```

**Train DistilBERT (faster baseline):**

```text
ACCELERATE_USE_CPU=1 PYTHONPATH=src python -m ticket_classifier.train_transformer \
 --model-name distilbert-base-uncased --max-len 128 \
 --train-n 11711 --val-n 1674 --epochs 3 --train-batch 4
```

**Quick micro-run (debug the training loop fast):**

```text
ACCELERATE_USE_CPU=1 PYTHONPATH=src python -m ticket_classifier.train_transformer \
 --model-name roberta-base --max-len 128 \
 --train-n 3000 --val-n 1000 --epochs 1 --train-batch 4
```

**Change learning rate / batch sizes:**

```text
ACCELERATE_USE_CPU=1 PYTHONPATH=src python -m ticket_classifier.train_transformer \
--model-name roberta-base --max-len 128 \
--train-n 11711 --val-n 1674 --epochs 3 --lr 1e-5 \--train-batch 4 --eval-batch 16
```

> Models are saved under: artifacts/transformer_runs/<run_name>/best_model

**üìà Evaluation**

Transformer models are evaluated using the same metrics as classical models.

Example performance (Top 5 classes):

**RoBERTa (fine-tuned):**

- **Accuracy:** ~80%

- **Weighted F1:** ~0.80

This represents an improvement over TF-IDF baselines.

## üöÄ Deployment & Inference

This project exposes a FastAPI inference service for ticket classification using a fine-tuned RoBERTa model hosted on Hugging Face Hub.

The model is not stored in this repository. It is downloaded automatically at runtime.

**üîπ Model Source (Hugging Face Hub)**

Model ID:

```
mahsabr/ticket-classifier-roberta-top5
```

The API loads the model using:

```
AutoTokenizer.from_pretrained(MODEL_ID)
AutoModelForSequenceClassification.from_pretrained(MODEL_ID)
```

‚úî No local artifacts required
‚úî No Hugging Face account required (public model)

### ‚ñ∂Ô∏è Run Locally (Python)

**Steps**

```
# Clone the repo

git clone https://github.com/<your-username>/TicketClassification.git
cd TicketClassification

# Create & activate virtual environment

python -m venv venv
source venv/bin/activate

# Install dependencies

pip install -r requirements.txt

# Set model source

export MODEL_ID=mahsabr/ticket-classifier-roberta-top5
export PYTHONPATH=src

# Start the API

uvicorn api.app:app --reload
```

API will be available at:

```
http://127.0.0.1:8000
```

### üê≥ Run with Docker (Recommended)

**Build the image**

```
docker build -t ticket-classifier-api .
```

**Run the container**

```
docker run -p 8000:8000 \
 -e MODEL_ID=mahsabr/ticket-classifier-roberta-top5 \
 ticket-classifier-api
```

**üîç Inference Example**

```
curl -X POST "http://127.0.0.1:8000/predict" \
 -H "Content-Type: application/json" \
 -d '{"text":"My credit card was charged twice and the bank refused to reverse it."}'
```

**Response:**

```
{
"text": "My credit card was charged twice and the bank refused to reverse it.",
"predicted_label": "Credit card or prepaid card",
"confidence": 0.98
}
```

**‚ù§Ô∏è Health Check**

```
curl http://127.0.0.1:8000/health
```

**Response:**

```
{"status":"ok"}
```

.

## üñ•Ô∏è Web Interface (HTML Landing Page)

![Web UI demo](assets/web_ui_demo.png)

In addition to the REST API, the project includes a simple HTML landing page for interactive predictions.

This allows users to:

- Paste a ticket/complaint text

- Click a button to run inference

- View the predicted category and confidence score instantly

> The UI is served directly by FastAPI (no separate frontend stack).

**üê≥ Docker Support**

The landing page is implemented using container-safe paths, ensuring it works consistently:

- Local development

- Docker containers

- Cloud deployments (e.g. Cloud Run)

> No extra configuration is required.

### ‚ñ∂Ô∏è Access the Web UI

When the API is running (locally or via Docker), open:

```
http://127.0.0.1:8000/
```

### How It Works

- The page is served from a static HTML file.

- User input is sent to the /predict endpoint via fetch

- The API returns:
  - predicted_label

  - confidence (softmax probability)

> This provides a lightweight demo experience without additional frontend tooling

## ‚úÖ Summary

- Fine-tuned transformer model hosted on Hugging Face Hub

- FastAPI inference service with confidence scores

- Model auto-downloads at startup (no local artifacts required)

- Dockerized for reproducible deployment

- Includes a web-based HTML landing page for interactive predictions

- Runs consistently locally, in Docker, and in cloud environments

## ‚òÅÔ∏è Cloud Deployment (Ready)

This Docker image is cloud-ready and can be deployed to:

- Google Cloud Run

- AWS ECS / App Runner

- Azure Container Apps

- Kubernetes

## Next Steps

Planned improvements:

1. Deploy to Cloud

2. CI pipeline (GitHub Actions)

3. Error analysis & label refinement

## Status

üöß Project is in progress
